Cyberbullying Toxic Analyzer

This project is a simple C-based text analyzer designed to detect toxic or abusive language in text files. It can load input files, remove stopwords, identify toxic words with severity scores, count word frequencies, analyze sentences, and generate a clear analysis report.

This project was completed as part of Group Coursework, following all required specifications: structured programming, file processing, tokenization, sorting, and reflective documentation.

üìÇ Repository Structure
File	Description
main.c	Main program source code
main.exe	Compiled executable (Windows)
stopwords.txt	List of words to ignore
toxicwords.txt	Toxic words + severity levels
cyber.txt	Sample testing file (from Kaggle dataset)
analysis_report.txt	Output generated by program
Report.docx / Report.pdf	Reflective report (included separately)
‚öôÔ∏è How to Compile and Run Using GCC (Command Line)

Verify GCC is installed:

gcc --version


Compile the program:

gcc main.c -o main.exe


Run the program:

./main.exe

üñ•Ô∏è Running in Visual Studio Code
1Ô∏è‚É£ Install the required tools:

C/C++ extension (Microsoft)

MinGW (Windows)

2Ô∏è‚É£ Open your project folder in VS Code

Ensure the following files are all in the same folder:

main.c

stopwords.txt

toxicwords.txt

cyber.txt

3Ô∏è‚É£ Compile in VS Code terminal:
gcc main.c -o main.exe

4Ô∏è‚É£ Run:
./main.exe


A menu-driven interface will appear.

üì• Required Input Files
File	Purpose
stopwords.txt	Words to ignore during analysis
toxicwords.txt	Toxic words with severity levels
cyber.txt	Input sample from Kaggle cyberbullying dataset

If any file is missing, the program may warn you or fail to analyze the text.

üì§ Program Output

The program generates:

‚úî analysis_report.txt

This contains:

Word counts

Sentence analysis

Toxic word detection summary

Top word frequencies

Severity breakdown

Clean formatted output

This file is saved automatically after running the program.

üß™ Dataset Used

We tested the program using the Cyberbullying Classification Dataset from Kaggle:

üîó https://www.kaggle.com/datasets/andrewmvd/cyberbullying-classification

This dataset provided real-world samples for testing toxic word detection accuracy.

üéâ Thank You for Viewing This Project

