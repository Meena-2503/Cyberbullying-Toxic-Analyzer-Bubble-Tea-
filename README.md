Cyberbullying Toxic Analyzer 
This project is a simple C-based text analyzer designed to detect toxic or abusive language in text files. It can load input files, remove stopwords, identify toxic words with severity scores, count word frequencies, analyze sentences, and generate a clear analysis report.
This project was completed as part of Group Coursework, following all required specifications: structured programming, file processing, tokenization, sorting, and reflective documentation.

üìÇ Repository Structure
File	Description
main.c	Main program source code
main.exe	Compiled executable (Windows)
stopwords.txt	List of words to ignore
toxicwords.txt	Toxic words + severity levels
cyber.txt	Sample testing file (from Kaggle dataset)
analysis_report.txt	Output generated by program
Report.pdf / Report.docx	Reflective report (included separately)

‚öôÔ∏è How to Compile and Run
Using GCC (Command Line)
1.	Verify GCC is installed:
gcc --version
2.	Compile the program:
gcc main.c -o main.exe
3.	Run the program:
./main.exe

üñ•Ô∏è Running in Visual Studio Code
1Ô∏è‚É£ Install the required tools:
‚Ä¢	C/C++ extension (Microsoft)
‚Ä¢	MinGW (for Windows users)
2Ô∏è‚É£ Open your project folder in VS Code
Ensure the following files are all in the same folder:
‚Ä¢	main.c
‚Ä¢	stopwords.txt
‚Ä¢	toxicwords.txt
‚Ä¢	cyber.txt
3Ô∏è‚É£ Compile in VS Code terminal:
gcc main.c -o main.exe
4Ô∏è‚É£ Run:
./main.exe
The menu-driven program will appear.

üì• Required Input Files
The program needs these files to run successfully:
File	Purpose
stopwords.txt	-Words to ignore during analysis
toxicwords.txt	-Toxic words with severity levels
cyber.txt	-Input sample from Kaggle cyberbullying dataset
If any file is missing, the program may warn you or fail to analyze the text.

üì§ Program Output
After running, the program generates:
‚úî analysis_report.txt
This contains:
‚Ä¢	Word counts
‚Ä¢	Sentence analysis
‚Ä¢	Toxic word detection summary
‚Ä¢	Top word frequencies
‚Ä¢	Severity breakdown
‚Ä¢	Clean formatted output
This output file is saved automatically.

üß™ Dataset Used
We tested the program using the Cyberbullying Classification Dataset from Kaggle:
üîó https://www.kaggle.com/datasets/andrewmvd/cyberbullying-classification
This dataset provided real-world text samples for evaluating toxic word detection accuracy.

üéâ Thank You for Viewing This Project
